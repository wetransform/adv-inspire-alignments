buildscript {
  repositories {
    maven {
      url 'https://artifactory.wetransform.to/artifactory/libs-release-local'
    }
    // maven {
    //   url 'https://artifactory.wetransform.to/artifactory/libs-snapshot-local'
    // }
  }
  dependencies {
    classpath 'to.wetransform.hale:gradle-hale-plugin:1.1.2'
  }
}

apply plugin: 'to.wetransform.hale'

hale {
  //cliVersion = '3.3.0'
  cliVersion = '3.4.0-SNAPSHOT' // for CSV Double binding support & CLI rewrite task
}

configurations.all {
  resolutionStrategy.cacheChangingModulesFor 5, 'minutes'
}

def json = new groovy.json.JsonSlurper()
def transformations = json.parse(file(
  project.hasProperty('transformationsFile') ? project.getProperty('transformationsFile') : 'transformations.json'))

task('transform-all') {
  description "Runs all transformation tasks."
  group 'Transformation'
}

if (tasks.findByPath('clean') == null) {
  // create clean task
  task clean() {
    description 'Deletes all transformation results.'
    group 'Transformation'
  }
}

tasks.clean.doLast {
  // add actions to clean task
  delete project.file('transformiert')
}

transformations.each { name, config ->
  if (config.enabled != null && !config.enabled) {
    return
  }

  String targetName = name
  File targetFolder = new File(project.file('transformiert'), targetName)
  File targetFile = new File(targetFolder, 'result.gml.gz')
  
  // add transformation task for each key
  task("transform-$name", type: hale.transform()) {
    // transformation project
    transformation = file(config.project)

    // project variables from config
    if (config.variables) {
      projectVariables << config.variables
    }

    // special case: Hauskoordinaten mapping
    boolean isHauskoordinaten = (name == 'ad-hk')

    if (config.model && !isHauskoordinaten) {
      // restrict to a specific model -> add generic filter
      filterArgs << '-filter'
      filterArgs << "CQL:modellart.AA_Modellart.advStandardModell = '${config.model}'"

      if (config.additionalModels) {
        // add filters for additional models (also accepting those)
        config.additionalModels.each { modelName ->
          filterArgs << '-filter'
          filterArgs << "CQL:modellart.AA_Modellart.advStandardModell = '${modelName}'"
        }
      }

      // add project variable
      projectVariables['ADV_MODELLART'] = config.model
    }

    // add sources
    if (!isHauskoordinaten) {
      // project w/ XML/GML sources
      
      // source folder
      def sourceFolder = project.file('quelldaten')
      if (config.sourceFolder) {
        // prefer source folder from definition
        sourceFolder = project.file(config.sourceFolder)
      }
      else {
        String folder

        if (config.model && project.hasProperty('defaultSourceFolder_' + config.model)) {
          // use custom default source folder for model if present
          folder = project.getProperty('defaultSourceFolder_' + config.model)
        }

        if (!folder && project.hasProperty('defaultSourceFolder')) {
          // otherwise use custom default source folder if present
          folder = project.getProperty('defaultSourceFolder')
        }

        if (folder) {
          sourceFolder = project.file(folder)
        }
      }

      // source CRS
      def sourceCRS = null
      if (config.sourceCRS) {
        // prefer source CRS from definition
        sourceCRS = config.sourceCRS;
      }
      else {
        if (config.model && project.hasProperty('defaultSourceCRS_' + config.model)) {
          // use custom default source folder for model if present
          sourceCRS = project.getProperty('defaultSourceCRS_' + config.model)
        }

        if (!sourceCRS && project.hasProperty('defaultSourceCRS')) {
          // otherwise use custom default source CRS if present
          sourceCRS = project.getProperty('defaultSourceCRS')
        }
      }

      // source definitions

      source(sourceFolder) {
        provider 'eu.esdihumboldt.hale.io.gml.reader'
        include '*.gz'
        setting 'contentType', 'eu.esdihumboldt.hale.io.xml.gzip'
        if (sourceCRS) {
          setting 'defaultSrs', "code:${sourceCRS}"
        }
      }

      source(sourceFolder) {
        provider 'eu.esdihumboldt.hale.io.gml.reader'
        include '*.gml'
        include '*.xml'
        setting 'contentType', 'org.eclipse.core.runtime.xml'
        if (sourceCRS) {
          setting 'defaultSrs', "code:${sourceCRS}"
        }
      }

      // filter duplicate objects based on GML ID (= AAA-ObjektID)
      filterArgs << '-exclude'
      filterArgs << '''groovy:
        def id = instance.p.id.value();
        boolean rejected = false;
        if (id) {
          withContext {
            def collect = _.context.collector(it);
            if (collect.ids.values().contains(id)) {
              _log.warn(\'Rejecting feature with duplicate id \' + id);
              rejected = true;
            }
            else {
              collect.ids << id;
            }
          }
        }

        rejected;
      '''
      // enable global filter context (to allow filtering duplicates across files)
      overallFilterContext = true 
    }
    else {
      // Hauskoordinaten project
      
      // file locations
      def hkSchluesselDatei = project.hasProperty('hkSchluesselDatei') ? project.getProperty('hkSchluesselDatei') : project.file('testdaten/Hauskoordinaten/schluessel_test.csv')
      def hkDatei = project.hasProperty('hkDatei') ? project.property('hkDatei') : project.file('testdaten/Hauskoordinaten/adressen_test.csv')

      // common settings
      def hkQuote = '"'
      def hkCharset = 'UTF-8'
      def hkDecimal = ','
      def hkSeparator = ';'
      def hkEscape = '\\'

      // define sources
      
      source(hkSchluesselDatei) {
        provider 'eu.esdihumboldt.hale.io.csv.reader.instance'

        setting 'typename', 'Schlüssel'
        setting 'skip', project.hasProperty('hkSchluesselSkipFirst') ? project.getProperty('hkSchluesselSkipFirst') : false

        setting 'charset', hkCharset
        setting 'quote', hkQuote
        setting 'decimal', hkDecimal
        setting 'separator', hkSeparator
        setting 'escape', hkEscape
      }

      source(hkDatei) {
        provider 'eu.esdihumboldt.hale.io.csv.reader.instance'

        setting 'typename', 'Hauskoordinaten'
        setting 'skip', project.hasProperty('hkSkipFirst') ? project.getProperty('hkSkipFirst') : false

        setting 'charset', hkCharset
        setting 'quote', hkQuote
        setting 'decimal', hkDecimal
        setting 'separator', hkSeparator
        setting 'escape', hkEscape
      }
    }

    // transformation target
    target(targetFile) {
      provider 'eu.esdihumboldt.hale.io.wfs.fc.write-2.0' // WFS 2 FC
      setting 'xml.pretty', true
      setting 'crs.epsg.prefix', 'http://www.opengis.net/def/crs/EPSG/0/'
      setting 'skipFeatureCount', true
      setting 'contentType', 'eu.esdihumboldt.hale.io.xml.gzip'
    }
    
    // XML schema validation
    validate('eu.esdihumboldt.hale.io.xml.validator')
    
    // folder for output and reports
    logFolder = targetFolder

    // general options
    printStacktrace = true
    trustGroovy = true

    // activate hale internal validation of transformed instances
    environment['HALE_TRANSFORMATION_INTERNAL_VALIDATION'] = 'true'
    
    // statistics and ...
    additionalArgs << '-statisticsOut'
    File statsFile = new File(targetFolder, 'statistics.json')
    additionalArgs << statsFile.absolutePath
    // ... success evaluation
    additionalArgs << '-successEvaluation'
    additionalArgs << project.file('success.groovy')

    // custom additional arguments
    if (config.additionalArgs && config.additionalArgs instanceof Collection) {
      additionalArgs.addAll(config.additionalArgs)
    }

    description "Runs a transformation based on the project ${transformation.name}."
    group 'Transformation'
  }

  tasks["transform-$name"].doFirst {
    targetFolder.mkdirs()
  }

  tasks['transform-all'].dependsOn("transform-$name")

  // validation
  
  task("validate-$name") {
    description "Validate the transformed data set $name (NOT IMPLEMENTED)"
    group 'Validation'
  }.doLast {
    // check if the target file exists
    // we don't add a dependency to the transform task by intention, so that
    // the validation can be run independently
    assert targetFile.exists()

    /*
     * XXX This task could be used to validate the result from the transform task
     *
     * - check reports
     *   - check if the transformation was successful and there were no errors
     *   - check if writing the result was successful and w/o errors
     *   - check if the internal validation report was successful and there were no warnings (e.g. Xlink references that were not resolved)
     *   - check if the schema validation was successful and there were no errors
     * - run a validator on the data (e.g. Schematron, INSPIRE validator, ...)
     */
    
    
  }

  // upload

  task("upload-$name", type: hale.cli()) {
    args = ['data', 'rewrite'];

    args << '--data'
    args << targetFile.absolutePath

    // use all feature types as mapping relevant types
    args << '--schema-setting'
    args << 'relevantMode=featureTypes'

    // suppress parsing geometries (no need to re-encode)
    args << '--data-setting'
    args << 'suppressParsingGeometry=true'

    args << '--target'
    args << project.findProperty('uploadWFS')
    args << '--target-writer'
    args << 'eu.esdihumboldt.hale.io.wfs.write.partitioned'
    // args << 'eu.esdihumboldt.hale.io.wfs.write'
    args << '--target-setting'
    args << 'wfsVersion=2.0.0'

    description "Upload the transformed data set $name to a WFS-T"
    group 'WFS-T upload'
  }
  tasks["upload-$name"].doFirst {
    // check if the target file exists
    // we don't add a dependency to the transform or validate task by intention,
    // so that the upload can be run independently
    assert targetFile.exists()

    // WFS-T address must be set
    assert project.hasProperty('uploadWFS')
  }

}

// add documentation tasks

task('doc') {
  description 'Generate mapping documentation (All variants).'
  group 'Documentation'
}

[
  [task: 'docHtml', cmd: 'export-doc', type: 'HTML'],
  [task: 'docExcel', cmd: 'export-table', type: 'Excel']
].each { item ->
  task(item.task, type: hale.cli()) {
    args = ['project', 'alignment', item.cmd]

    description "Generate mapping documentation (${item.type})."
    group 'Documentation'
  }

  tasks['doc'].dependsOn(item.task)
}

tasks.docHtml.doLast {
  // replace halejs URLs with one that supports https
  print 'Use https URLs to load halejs...'

  // find documentation files
  def files = fileTree(dir: projectDir, include: 'annex*/mappings/**/*.halex.svg.html')

  files.each { file ->
    def text = file.text
    file.text = text.replaceAll(
      java.util.regex.Pattern.quote('http://build-artifacts.wetransform.to'),
      'https://s3-eu-central-1.amazonaws.com/build-artifacts.wetransform.to')
  }

  println 'done'
}

// add tasks for creating derived projects

task('derive-all') {
  description "Creates all derived projects."
  group 'Derived transformation projects'
}

transformations.each { name, config ->
  if (config.enabled != null && !config.enabled) {
    return
  }

  // create a task to derive a project for every model
  // association of a project

  if (config.model && config.deriveProject != false) {
    // determine model definition
    def modelDefinition
    switch(config.model) {
    case 'DLKM':
      modelDefinition = file('dlkm.model.json')
      break
    case 'Basis-DLM':
      modelDefinition = file('basis-dlm.model.json')
      break
    case 'DLM50':
      modelDefinition = file('dlm-50.model.json')
      break
    case 'DLM250':
      modelDefinition = file('dlm-250.model.json')
      break
    case 'DLM1000':
      modelDefinition = file('dlm-1000.model.json')
      break
    }

    def projectFile = file(config.project)

    if (modelDefinition) {
      def filterDir = new File(buildDir, 'filter')
      def filterFile = new File(filterDir, name + '-' + modelDefinition.name)

      // task to prepare filter definition
      task("prepare-filter-$name").doLast {
        // copy model definition
        filterDir.mkdirs()
        copy {
          from modelDefinition
          into filterDir
          rename {
            filterFile.name
          }
        }

        if (config.deriveFilter) {
          // merge filter with model definition
          def filterDef = new groovy.json.JsonSlurper().parse(filterFile)
          filterDef.putAll(config.deriveFilter)
          filterFile.text = groovy.json.JsonOutput.prettyPrint(
            groovy.json.JsonOutput.toJson(filterDef))
        }
      }

      // task to derive project
      task("derive-$name", type: hale.cli(), dependsOn: "prepare-filter-$name") {
        args = [
          'project',
          'alignment',
          'filter',
          '--name',
          config.model,
          '--json-filter',
          filterFile,
          '--skip-empty',
          '--skip-no-type-cells',
          '--prepend-description',
          "***Automatisch generierte Variante des Projekts für das ${config.model} Modell***"
        ]

        // allow customizing the behavior (also from projects using this as sub-project)
        if (!project.ext.has('deriveProjectUseCopy') || !project.ext.get('deriveProjectUseCopy')) {
          args << '--use-base-alignment'
        }

        args << projectFile

        requiresHaleCli = true

        description "Generate derived project for model ${config.model} of project ${projectFile.name}"
        group 'Derived transformation projects'
      }

      tasks['derive-all'].dependsOn("derive-$name")
    }
  }
}

/*
 * Gradle wrapper
 */
task wrapper(type: Wrapper) {
  gradleVersion = '2.14'
}
